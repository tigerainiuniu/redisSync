#!/usr/bin/env python3
"""
RedisåŒæ­¥æœåŠ¡

æ”¯æŒä¸€å¯¹å¤šRediså®ä¾‹çš„æŒç»­åŒæ­¥æœåŠ¡ã€‚
"""

import asyncio
import logging
import signal
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import yaml
import redis
from pathlib import Path

from .connection_manager import RedisConnectionManager
from .migration_orchestrator import MigrationOrchestrator, MigrationConfig, MigrationType
from .web_ui import WebUI
from .unified_incremental_service import UnifiedIncrementalService


@dataclass
class SyncTarget:
    """åŒæ­¥ç›®æ ‡é…ç½®"""
    name: str
    host: str
    port: int
    password: Optional[str] = None
    db: int = 0
    ssl: bool = False
    enabled: bool = True
    connection_config: Dict[str, Any] = None


@dataclass
class SyncStats:
    """åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
    total_synced: int = 0
    total_failed: int = 0
    last_sync_time: Optional[float] = None
    last_error: Optional[str] = None
    consecutive_failures: int = 0
    is_healthy: bool = True


class RedisSyncService:
    """RedisåŒæ­¥æœåŠ¡"""
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = self._load_config()
        self.logger = self._setup_logging()
        
        # æœåŠ¡çŠ¶æ€
        self.running = False
        self.shutdown_event = threading.Event()
        
        # è¿æ¥ç®¡ç†å™¨å’Œç¼–æ’å™¨
        self.source_conn = None
        self.target_connections: Dict[str, RedisConnectionManager] = {}
        self.orchestrators: Dict[str, MigrationOrchestrator] = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats: Dict[str, SyncStats] = {}
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(
            max_workers=self.config['service']['performance']['max_workers']
        )

        # åŒæ­¥ä»»åŠ¡
        self.sync_tasks: List[threading.Thread] = []

        # ç»Ÿä¸€å¢é‡åŒæ­¥æœåŠ¡
        self.incremental_service: Optional[UnifiedIncrementalService] = None

        # Web UI
        self.web_ui = None
        if self.config.get('web_ui', {}).get('enabled', True):
            web_config = self.config['web_ui']
            self.web_ui = WebUI(
                self,
                host=web_config.get('host', '0.0.0.0'),
                port=web_config.get('port', 8080)
            )

        # è®°å½•å¯åŠ¨æ—¶é—´
        self.start_time = time.time()

        self.logger.info("RedisåŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config['service']['logging']

        # é…ç½® root loggerï¼Œè¿™æ ·æ‰€æœ‰æ¨¡å—çš„æ—¥å¿—éƒ½ä¼šè¾“å‡º
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, log_config['level']))

        # æ¸…é™¤å·²æœ‰çš„ handlers
        root_logger.handlers.clear()

        # æ–‡ä»¶å¤„ç†å™¨
        from logging.handlers import RotatingFileHandler
        file_handler = RotatingFileHandler(
            log_config['file'],
            maxBytes=log_config['max_size'],
            backupCount=log_config['backup_count']
        )
        file_handler.setFormatter(logging.Formatter(log_config['format']))
        root_logger.addHandler(file_handler)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(logging.Formatter(log_config['format']))
        root_logger.addHandler(console_handler)

        # è¿”å›æœåŠ¡ä¸“ç”¨çš„ logger
        logger = logging.getLogger('redis-sync-service')

        return logger
    
    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            self.logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
            self.stop()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def _connect_source(self) -> bool:
        """è¿æ¥æºRedis"""
        try:
            source_config = self.config['source']
            # æ„å»ºè¿æ¥å‚æ•°
            conn_params = {
                'host': source_config['host'],
                'port': source_config['port'],
                'password': source_config.get('password'),
                'db': source_config.get('db', 0),
                'ssl': source_config.get('ssl', False),
                'socket_timeout': source_config.get('socket_timeout', 30),
                'socket_connect_timeout': source_config.get('socket_connect_timeout', 30),
                'decode_responses': True
            }

            # socket_keepalive åœ¨æŸäº›ç³»ç»Ÿä¸Šå¯èƒ½ä¸æ”¯æŒï¼Œå°è¯•æ·»åŠ 
            if source_config.get('socket_keepalive', False):
                try:
                    conn_params['socket_keepalive'] = True
                except:
                    pass  # å¦‚æœä¸æ”¯æŒå°±å¿½ç•¥

            self.source_conn = redis.Redis(**conn_params)
            
            # æµ‹è¯•è¿æ¥
            self.source_conn.ping()
            self.logger.info(f"æºRedisè¿æ¥æˆåŠŸ: {source_config['host']}:{source_config['port']}")
            return True
            
        except Exception as e:
            self.logger.error(f"æºRedisè¿æ¥å¤±è´¥: {e}")
            return False
    
    def _connect_targets(self) -> bool:
        """è¿æ¥æ‰€æœ‰ç›®æ ‡Redis"""
        success = True
        
        for target_config in self.config['targets']:
            if not target_config.get('enabled', True):
                continue
                
            target_name = target_config['name']
            
            try:
                # åˆ›å»ºè¿æ¥ç®¡ç†å™¨
                conn_manager = RedisConnectionManager()
                
                # è¿æ¥æºRedis
                conn_manager.connect_source(
                    host=self.config['source']['host'],
                    port=self.config['source']['port'],
                    password=self.config['source'].get('password'),
                    db=self.config['source'].get('db', 0)
                )
                
                # è¿æ¥ç›®æ ‡Redis
                conn_manager.connect_target(
                    host=target_config['host'],
                    port=target_config['port'],
                    password=target_config.get('password'),
                    db=target_config.get('db', 0)
                )
                
                # åˆ›å»ºç¼–æ’å™¨
                orchestrator = MigrationOrchestrator(conn_manager)

                # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆä¼ é€’scan_counté…ç½®ï¼‰
                scan_count = self.config['service']['performance'].get('scan_count', 10000)
                orchestrator.initialize_handlers_with_config(scan_count=scan_count)

                self.target_connections[target_name] = conn_manager
                self.orchestrators[target_name] = orchestrator
                self.stats[target_name] = SyncStats()
                
                self.logger.info(f"ç›®æ ‡Redisè¿æ¥æˆåŠŸ: {target_name} ({target_config['host']}:{target_config['port']})")
                
            except Exception as e:
                self.logger.error(f"ç›®æ ‡Redisè¿æ¥å¤±è´¥ {target_name}: {e}")
                success = False
        
        return success
    
    def _create_sync_config(self) -> MigrationConfig:
        """åˆ›å»ºåŒæ­¥é…ç½®"""
        sync_config = self.config['sync']

        if sync_config['mode'] == 'full':
            migration_type = MigrationType.FULL
        elif sync_config['mode'] == 'incremental':
            migration_type = MigrationType.INCREMENTAL
        else:  # hybrid
            migration_type = MigrationType.FULL  # å…ˆå…¨é‡ï¼Œåé¢ä¼šå¯åŠ¨å¢é‡

        # è·å–full_syncé…ç½®
        full_sync_config = sync_config.get('full_sync', {})

        config = MigrationConfig(
            migration_type=migration_type,
            key_pattern=full_sync_config.get('key_pattern', '*'),
            batch_size=full_sync_config.get('batch_size', 1000),
            scan_count=full_sync_config.get('scan_count', 10000),
            preserve_ttl=full_sync_config.get('preserve_ttl', True),
            verify_migration=full_sync_config.get('verify_migration', True),
            verify_mode=full_sync_config.get('verify_mode', 'fast'),
            verify_sample_size=full_sync_config.get('verify_sample_size', 100)
        )

        return config
    
    def _perform_full_sync(self, target_name: str) -> bool:
        """æ‰§è¡Œå…¨é‡åŒæ­¥"""
        try:
            start_time = time.time()
            orchestrator = self.orchestrators[target_name]
            config = self._create_sync_config()

            self.logger.info(f"ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥: {target_name} (æ—¶é—´æˆ³: {start_time:.2f})")

            # è®°å½•å¼€å§‹è¿ç§»å‰çš„æ—¶é—´
            before_migrate = time.time()
            result = orchestrator.migrate(config)
            after_migrate = time.time()

            # è®¡ç®—å®é™…è¿ç§»è€—æ—¶
            actual_migrate_time = after_migrate - before_migrate

            # è°ƒè¯•æ—¥å¿—
            self.logger.debug(f"è¿ç§»ç»“æœ: success={result.get('success')}, stats keys={list(result.get('statistics', {}).keys())}")
            self.logger.debug(f"{target_name} å®é™…è¿ç§»è€—æ—¶: {actual_migrate_time:.2f}ç§’")

            if result['success']:
                stats = result.get('statistics', {})
                # ä½¿ç”¨å®é™…è¿ç§»æ—¶é—´ï¼Œè€Œä¸æ˜¯ä»å‡½æ•°å¼€å§‹çš„æ—¶é—´
                elapsed = actual_migrate_time
                migrated_keys = stats.get('migrated_keys', 0)

                # è·å–éªŒè¯æ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
                verification_time = result.get('verification_time', 0)

                self.stats[target_name].total_synced += migrated_keys
                self.stats[target_name].last_sync_time = time.time()
                self.stats[target_name].consecutive_failures = 0
                self.stats[target_name].is_healthy = True

                # å•è¡Œæ—¥å¿—ï¼Œé¿å…å¤šè¡Œæ˜¾ç¤ºé—®é¢˜
                speed = migrated_keys / elapsed if elapsed > 0 else 0

                # å¦‚æœæœ‰éªŒè¯æ—¶é—´ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                if verification_time > 0:
                    migrate_only_time = elapsed - verification_time
                    migrate_speed = migrated_keys / migrate_only_time if migrate_only_time > 0 else 0
                    self.logger.info(
                        f"âœ… {target_name} å…¨é‡åŒæ­¥å®Œæˆ - "
                        f"é”®æ•°: {migrated_keys}, "
                        f"æ€»è€—æ—¶: {elapsed:.2f}ç§’ (è¿ç§»: {migrate_only_time:.2f}ç§’, éªŒè¯: {verification_time:.2f}ç§’), "
                        f"è¿ç§»é€Ÿåº¦: {migrate_speed:.0f} é”®/ç§’"
                    )
                else:
                    self.logger.info(
                        f"âœ… {target_name} å…¨é‡åŒæ­¥å®Œæˆ - "
                        f"é”®æ•°: {migrated_keys}, "
                        f"è€—æ—¶: {elapsed:.2f}ç§’, "
                        f"é€Ÿåº¦: {speed:.0f} é”®/ç§’"
                    )
                return True
            else:
                raise Exception(result.get('error', 'æœªçŸ¥é”™è¯¯'))
                
        except Exception as e:
            self.stats[target_name].total_failed += 1
            self.stats[target_name].last_error = str(e)
            self.stats[target_name].consecutive_failures += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ‡è®°ä¸ºä¸å¥åº·
            max_failures = self.config['service']['failover']['max_failures']
            if self.stats[target_name].consecutive_failures >= max_failures:
                self.stats[target_name].is_healthy = False
                self.logger.error(f"ç›®æ ‡ {target_name} æ ‡è®°ä¸ºä¸å¥åº·ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {self.stats[target_name].consecutive_failures}")
            
            self.logger.error(f"å…¨é‡åŒæ­¥å¤±è´¥: {target_name}, é”™è¯¯: {e}")
            return False
    
    def _perform_incremental_sync(self, target_name: str) -> bool:
        """æ‰§è¡Œå¢é‡åŒæ­¥"""
        try:
            orchestrator = self.orchestrators[target_name]
            
            # åˆ›å»ºå¢é‡åŒæ­¥é…ç½®
            inc_config = self.config['sync']['incremental_sync']
            config = MigrationConfig(
                migration_type=MigrationType.INCREMENTAL,
                key_pattern=inc_config.get('key_pattern', '*'),
                max_changes_per_sync=inc_config.get('max_changes_per_sync', 10000),
                continuous_sync=False  # ä¸€æ¬¡æ€§å¢é‡åŒæ­¥
            )
            
            result = orchestrator.migrate(config)

            if result['success']:
                stats = result.get('statistics', {})
                changed_keys = stats.get('changed_keys', 0)
                synced_keys = stats.get('synced_keys', 0)
                failed_keys = stats.get('failed_keys', 0)

                if changed_keys > 0:
                    self.stats[target_name].total_synced += synced_keys
                    self.logger.info(f"âœ… å¢é‡åŒæ­¥å®Œæˆ: {target_name}")
                    self.logger.info(f"   å˜æ›´é”®æ•°: {changed_keys}")
                    self.logger.info(f"   åŒæ­¥æˆåŠŸ: {synced_keys}")
                    self.logger.info(f"   åŒæ­¥å¤±è´¥: {failed_keys}")
                else:
                    self.logger.debug(f"âœ“ å¢é‡åŒæ­¥: {target_name}, æ— å˜æ›´")

                self.stats[target_name].last_sync_time = time.time()
                self.stats[target_name].consecutive_failures = 0
                self.stats[target_name].is_healthy = True

                return True
            else:
                error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                self.logger.error(f"âŒ å¢é‡åŒæ­¥å¤±è´¥: {target_name}, é”™è¯¯: {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            self.stats[target_name].total_failed += 1
            self.stats[target_name].last_error = str(e)
            self.stats[target_name].consecutive_failures += 1

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ‡è®°ä¸ºä¸å¥åº·
            max_failures = self.config['service']['failover']['max_failures']
            if self.stats[target_name].consecutive_failures >= max_failures:
                self.stats[target_name].is_healthy = False
                self.logger.error(f"âš ï¸  ç›®æ ‡ {target_name} æ ‡è®°ä¸ºä¸å¥åº·ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {self.stats[target_name].consecutive_failures}")

            self.logger.error(f"âŒ å¢é‡åŒæ­¥å¤±è´¥: {target_name}, é”™è¯¯: {e}", exc_info=True)
            return False

    def _scan_source_for_changes(self) -> List[str]:
        """ç»Ÿä¸€æ‰«ææºRedisï¼Œæ£€æµ‹å˜æ›´çš„é”®ï¼ˆåªæ‰«æä¸€æ¬¡ï¼‰"""
        try:
            inc_config = self.config['sync']['incremental_sync']
            key_pattern = inc_config.get('key_pattern', '*')
            max_changes = inc_config.get('max_changes_per_sync', 10000)

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªorchestratoræ¥æ£€æµ‹å˜æ›´ï¼ˆå®ƒä»¬å…±äº«åŒä¸€ä¸ªæºï¼‰
            first_orchestrator = next(iter(self.orchestrators.values()))

            # è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            last_sync_time = min(
                (stats.last_sync_time for stats in self.stats.values() if stats.last_sync_time),
                default=time.time() - 30  # é»˜è®¤30ç§’å‰
            )

            self.logger.debug(f"ğŸ” æ‰«ææºRedisæ£€æµ‹å˜æ›´ï¼ˆä¸Šæ¬¡åŒæ­¥: {time.time() - last_sync_time:.1f}ç§’å‰ï¼‰")

            # è°ƒç”¨å¢é‡å¤„ç†å™¨æ£€æµ‹å˜æ›´
            changed_keys = first_orchestrator.incremental_migration_handler._detect_changes_by_idle_time(
                key_pattern=key_pattern,
                key_types=inc_config.get('key_types'),
                since_timestamp=last_sync_time,
                max_changes=max_changes
            )

            if changed_keys:
                self.logger.info(f"âœ“ æ£€æµ‹åˆ° {len(changed_keys)} ä¸ªå˜æ›´é”®")
            else:
                self.logger.debug(f"âœ“ æ— å˜æ›´é”®")

            return changed_keys

        except Exception as e:
            self.logger.error(f"âŒ æ‰«ææºRediså¤±è´¥: {e}", exc_info=True)
            return []

    def _sync_keys_to_target(self, target_name: str, keys: List[str]) -> bool:
        """å°†æŒ‡å®šçš„é”®åŒæ­¥åˆ°ç›®æ ‡ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨Pipelineæ‰¹é‡æ“ä½œï¼‰"""
        if not keys:
            return True

        try:
            orchestrator = self.orchestrators[target_name]
            source_client = orchestrator.connection_manager.source_client
            target_client = orchestrator.connection_manager.target_client

            synced = 0
            failed = 0

            # è¶…å¤§æ‰¹é‡å¤„ç†ï¼ˆå°æ•°æ®é›†ä¸€æ¬¡æ€§å¤„ç†ï¼‰
            batch_size = min(len(keys), 500)  # æœ€å¤§500ä¸ªé”®/æ‰¹
            total_batches = (len(keys) + batch_size - 1) // batch_size

            for batch_idx in range(0, len(keys), batch_size):
                batch_keys = keys[batch_idx:batch_idx + batch_size]
                current_batch = batch_idx // batch_size + 1

                try:
                    # ä½¿ç”¨æ— äº‹åŠ¡Pipelineæ‰¹é‡è¯»å–ï¼ˆæ›´å¿«ï¼‰
                    pipe = source_client.pipeline(transaction=False)
                    for key in batch_keys:
                        pipe.dump(key)
                        pipe.pttl(key)

                    results = pipe.execute()

                    # å¿«é€Ÿè§£æï¼ˆä½¿ç”¨å…ƒç»„è€Œä¸æ˜¯å­—å…¸ï¼‰
                    entries = []
                    for i in range(0, len(results), 2):
                        dump_data = results[i]
                        ttl = results[i + 1]

                        if dump_data is not None:
                            entries.append((
                                batch_keys[i // 2],
                                ttl if ttl > 0 else 0,
                                dump_data
                            ))

                    # ä½¿ç”¨æ— äº‹åŠ¡Pipelineæ‰¹é‡å†™å…¥ï¼ˆæ›´å¿«ï¼‰
                    if entries:
                        pipe = target_client.pipeline(transaction=False)
                        for key, ttl, data in entries:
                            pipe.restore(key, ttl, data, replace=True)

                        pipe.execute()
                        synced += len(entries)

                except Exception as e:
                    failed += len(batch_keys)
                    self.logger.warning(
                        f"âš ï¸  æ‰¹æ¬¡ {current_batch}/{total_batches} åŒæ­¥å¤±è´¥: {e}"
                    )

            # æ›´æ–°ç»Ÿè®¡
            self.stats[target_name].total_synced += synced
            self.stats[target_name].total_failed += failed
            self.stats[target_name].last_sync_time = time.time()

            if failed == 0:
                self.stats[target_name].consecutive_failures = 0
                self.stats[target_name].is_healthy = True

            self.logger.info(
                f"âœ… {target_name}: åŒæ­¥å®Œæˆ - æˆåŠŸ {synced} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ª"
            )

            return True

        except Exception as e:
            self.logger.error(f"âŒ åŒæ­¥åˆ°ç›®æ ‡ {target_name} å¤±è´¥: {e}", exc_info=True)
            self.stats[target_name].total_failed += len(keys)
            self.stats[target_name].consecutive_failures += 1

            max_failures = self.config['service']['failover']['max_failures']
            if self.stats[target_name].consecutive_failures >= max_failures:
                self.stats[target_name].is_healthy = False

            return False

    def _perform_unified_incremental_sync(self) -> bool:
        """æ‰§è¡Œç»Ÿä¸€å¢é‡åŒæ­¥ï¼ˆæ‰«æä¸€æ¬¡ï¼ŒåŒæ­¥åˆ°æ‰€æœ‰ç›®æ ‡ï¼‰"""
        try:
            # 1. æ‰«æä¸€æ¬¡æºRedis
            changed_keys = self._scan_source_for_changes()

            if not changed_keys:
                self.logger.debug("âœ“ ç»Ÿä¸€å¢é‡åŒæ­¥: æ— å˜æ›´")
                return True

            # 2. å¹¶è¡ŒåŒæ­¥åˆ°æ‰€æœ‰å¥åº·çš„ç›®æ ‡
            healthy_targets = [
                name for name, stats in self.stats.items()
                if stats.is_healthy
            ]

            if not healthy_targets:
                self.logger.warning("âš ï¸  æ²¡æœ‰å¥åº·çš„ç›®æ ‡å¯ä»¥åŒæ­¥")
                return False

            self.logger.info(f"â‡‰ å¹¶è¡ŒåŒæ­¥ {len(changed_keys)} ä¸ªé”®åˆ° {len(healthy_targets)} ä¸ªç›®æ ‡")

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŒæ­¥
            futures = []
            for target_name in healthy_targets:
                future = self.executor.submit(
                    self._sync_keys_to_target,
                    target_name,
                    changed_keys
                )
                futures.append((target_name, future))

            # ç­‰å¾…æ‰€æœ‰åŒæ­¥å®Œæˆ
            success_count = 0
            for target_name, future in futures:
                try:
                    if future.result(timeout=300):  # 5åˆ†é’Ÿè¶…æ—¶
                        success_count += 1
                        self.logger.info(f"  âœ“ {target_name}: åŒæ­¥å®Œæˆ")
                    else:
                        self.logger.warning(f"  âœ— {target_name}: åŒæ­¥å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"  âœ— {target_name}: {e}")

            self.logger.info(f"âœ“ ç»Ÿä¸€åŒæ­¥å®Œæˆ: {success_count}/{len(healthy_targets)} ä¸ªç›®æ ‡æˆåŠŸ")
            return success_count > 0

        except Exception as e:
            self.logger.error(f"âŒ ç»Ÿä¸€å¢é‡åŒæ­¥å¤±è´¥: {e}", exc_info=True)
            return False

    def _unified_sync_coordinator(self):
        """ç»Ÿä¸€åŒæ­¥åè°ƒå™¨ï¼ˆæ‰«æä¸€æ¬¡æºï¼ŒåŒæ­¥åˆ°æ‰€æœ‰ç›®æ ‡ï¼‰"""
        self.logger.info("ğŸš€ å¯åŠ¨ç»Ÿä¸€åŒæ­¥åè°ƒå™¨")

        # æ ¹æ®æ¨¡å¼æ‰§è¡Œåˆå§‹åŒæ­¥
        sync_mode = self.config['sync']['mode']
        self.logger.info(f"ğŸ“‹ åŒæ­¥æ¨¡å¼: {sync_mode}")

        # å…¨é‡åŒæ­¥é˜¶æ®µï¼ˆå¹¶è¡Œæ‰§è¡Œåˆ°æ‰€æœ‰ç›®æ ‡ï¼‰
        if sync_mode in ['full', 'hybrid']:
            self.logger.info("ğŸ”„ å¼€å§‹å…¨é‡åŒæ­¥é˜¶æ®µ")
            target_count = len(self.target_connections)
            self.logger.info(f"   åŒæ­¥ç›®æ ‡æ•°: {target_count}")
            self.logger.info(f"   ä¼˜åŒ–æ¨¡å¼: å¹¶è¡Œå…¨é‡åŒæ­¥")

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œå…¨é‡åŒæ­¥
            full_sync_start = time.time()
            with ThreadPoolExecutor(max_workers=min(target_count, 8)) as executor:
                futures = {}
                submit_times = {}
                for target_name in self.target_connections.keys():
                    submit_time = time.time()
                    self.logger.info(f"  â†’ å¯åŠ¨å…¨é‡åŒæ­¥: {target_name} (æäº¤æ—¶é—´: {submit_time:.2f})")
                    future = executor.submit(self._perform_full_sync, target_name)
                    futures[future] = target_name
                    submit_times[target_name] = submit_time

                # ç­‰å¾…æ‰€æœ‰åŒæ­¥å®Œæˆ
                for future in futures:
                    target_name = futures[future]
                    try:
                        success = future.result()
                        complete_time = time.time()
                        wait_time = complete_time - submit_times[target_name]
                        if success:
                            self.logger.info(f"  âœ… {target_name} å…¨é‡åŒæ­¥å®Œæˆ (ä»æäº¤åˆ°å®Œæˆ: {wait_time:.2f}ç§’)")
                        else:
                            self.logger.error(f"  âŒ {target_name} å…¨é‡åŒæ­¥å¤±è´¥")
                    except Exception as e:
                        self.logger.error(f"  âŒ {target_name} å…¨é‡åŒæ­¥å¼‚å¸¸: {e}")

            full_sync_total = time.time() - full_sync_start
            self.logger.info(f"âœ… å…¨é‡åŒæ­¥é˜¶æ®µå®Œæˆï¼Œæ€»è€—æ—¶: {full_sync_total:.2f}ç§’")

            self.logger.info("âœ… å…¨é‡åŒæ­¥é˜¶æ®µå®Œæˆ")

        # å¢é‡åŒæ­¥é˜¶æ®µ
        if sync_mode in ['incremental', 'hybrid']:
            inc_config = self.config['sync']['incremental_sync']
            if inc_config.get('enabled', True):
                inc_method = inc_config.get('method', 'scan')

                # æ£€æŸ¥å¢é‡åŒæ­¥æ¨¡å¼
                if inc_method in ['sync', 'psync']:
                    # ä½¿ç”¨ SYNC/PSYNC å®æ—¶å¤åˆ¶æ¨¡å¼
                    self.logger.info(f"ğŸš€ ä½¿ç”¨ {inc_method.upper()} å®æ—¶å¤åˆ¶æ¨¡å¼")
                    self._start_realtime_replication(inc_method, inc_config)
                else:
                    # ä½¿ç”¨ SCAN è½®è¯¢æ¨¡å¼
                    sync_interval = inc_config.get('interval', 5)
                    target_count = len(self.target_connections)

                    self.logger.info(f"â±ï¸  å¢é‡åŒæ­¥é—´éš”: {sync_interval} ç§’")
                    self.logger.info(f"âœ¨ ä¼˜åŒ–æ¨¡å¼ï¼šæ‰«æä¸€æ¬¡æºï¼Œå¹¶è¡ŒåŒæ­¥åˆ° {target_count} ä¸ªç›®æ ‡")

                    sync_count = 0
                    while self.running and not self.shutdown_event.is_set():
                        try:
                            sync_count += 1

                            # æ£€æŸ¥ä¸å¥åº·çš„ç›®æ ‡
                            unhealthy_targets = [
                                name for name, stats in self.stats.items()
                                if not stats.is_healthy
                            ]

                            if unhealthy_targets:
                                self.logger.warning(f"âš ï¸  å‘ç° {len(unhealthy_targets)} ä¸ªä¸å¥åº·çš„ç›®æ ‡: {unhealthy_targets}")
                                recovery_delay = self.config['service']['failover']['recovery_delay']

                                # å°è¯•æ¢å¤ä¸å¥åº·çš„ç›®æ ‡
                                for target_name in unhealthy_targets:
                                    self.logger.info(f"ğŸ”„ å°è¯•æ¢å¤ç›®æ ‡: {target_name}")
                                    if self._perform_full_sync(target_name):
                                        self.logger.info(f"  âœ“ ç›®æ ‡ {target_name} æ¢å¤æˆåŠŸ")
                                        self.stats[target_name].is_healthy = True
                                        self.stats[target_name].consecutive_failures = 0
                                    else:
                                        self.logger.warning(f"  âœ— ç›®æ ‡ {target_name} æ¢å¤å¤±è´¥")

                            # ç»Ÿä¸€æ‰§è¡Œå¢é‡åŒæ­¥
                            self.logger.info(f"ğŸ”„ [{sync_count}] å¼€å§‹ç»Ÿä¸€å¢é‡åŒæ­¥")
                            self._perform_unified_incremental_sync()

                            # ç­‰å¾…ä¸‹æ¬¡åŒæ­¥
                            self.logger.info(f"â³ ç­‰å¾… {sync_interval} ç§’åè¿›è¡Œä¸‹æ¬¡åŒæ­¥...")
                            if self.shutdown_event.wait(sync_interval):
                                break

                        except Exception as e:
                            self.logger.error(f"âŒ ç»Ÿä¸€åŒæ­¥åè°ƒå™¨å¼‚å¸¸: {e}", exc_info=True)
                            if self.shutdown_event.wait(sync_interval):
                                break

        self.logger.info("ğŸ›‘ ç»Ÿä¸€åŒæ­¥åè°ƒå™¨ç»“æŸ")

    def _start_realtime_replication(self, mode: str, config: Dict[str, Any]):
        """å¯åŠ¨å®æ—¶å¤åˆ¶ï¼ˆSYNC/PSYNCï¼‰"""
        try:
            # åˆ›å»ºç»Ÿä¸€å¢é‡åŒæ­¥æœåŠ¡
            self.incremental_service = UnifiedIncrementalService(
                mode=mode,
                source_conn=self.source_conn,
                target_connections=self.target_connections,
                config=config
            )

            # å¯åŠ¨æœåŠ¡
            self.incremental_service.start()

            # ç­‰å¾…ç›´åˆ°æœåŠ¡åœæ­¢
            while self.running and not self.shutdown_event.is_set():
                self.shutdown_event.wait(1)

            # åœæ­¢æœåŠ¡
            if self.incremental_service:
                self.incremental_service.stop()

        except Exception as e:
            self.logger.error(f"âŒ {mode.upper()} å¤åˆ¶å¤±è´¥: {e}", exc_info=True)

    def start(self) -> bool:
        """å¯åŠ¨åŒæ­¥æœåŠ¡"""
        self.logger.info("å¯åŠ¨RedisåŒæ­¥æœåŠ¡...")
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        self._setup_signal_handlers()
        
        # è¿æ¥Rediså®ä¾‹
        if not self._connect_source():
            return False
        
        if not self._connect_targets():
            return False
        
        if not self.target_connections:
            self.logger.error("æ²¡æœ‰å¯ç”¨çš„ç›®æ ‡Rediså®ä¾‹")
            return False
        
        # å¯åŠ¨Web UI
        if self.web_ui:
            try:
                self.web_ui.start()
            except Exception as e:
                self.logger.warning(f"Web UIå¯åŠ¨å¤±è´¥: {e}")

        # å¯åŠ¨ç»Ÿä¸€åŒæ­¥åè°ƒå™¨
        self.running = True

        # å¯åŠ¨ä¸€ä¸ªç»Ÿä¸€çš„åŒæ­¥åè°ƒå™¨çº¿ç¨‹ï¼ˆè€Œä¸æ˜¯ä¸ºæ¯ä¸ªç›®æ ‡å¯åŠ¨çº¿ç¨‹ï¼‰
        coordinator_thread = threading.Thread(
            target=self._unified_sync_coordinator,
            name="unified-sync-coordinator",
            daemon=True
        )
        coordinator_thread.start()
        self.sync_tasks.append(coordinator_thread)

        self.logger.info(f"âœ… RedisåŒæ­¥æœåŠ¡å¯åŠ¨æˆåŠŸ")
        self.logger.info(f"   åŒæ­¥ç›®æ ‡æ•°: {len(self.target_connections)}")
        self.logger.info(f"   ä¼˜åŒ–æ¨¡å¼: ç»Ÿä¸€æ‰«æ + å¹¶è¡Œåˆ†å‘")
        return True
    
    def stop(self):
        """åœæ­¢åŒæ­¥æœåŠ¡"""
        self.logger.info("åœæ­¢RedisåŒæ­¥æœåŠ¡...")
        
        self.running = False
        self.shutdown_event.set()
        
        # ç­‰å¾…æ‰€æœ‰åŒæ­¥ä»»åŠ¡ç»“æŸ
        for thread in self.sync_tasks:
            thread.join(timeout=30)
        
        # å…³é—­è¿æ¥
        for conn_manager in self.target_connections.values():
            try:
                conn_manager.close()
            except:
                pass
        
        if self.source_conn:
            try:
                self.source_conn.close()
            except:
                pass
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)

        # åœæ­¢Web UI
        if self.web_ui:
            try:
                self.web_ui.stop()
            except Exception as e:
                self.logger.warning(f"Web UIåœæ­¢å¤±è´¥: {e}")

        self.logger.info("RedisåŒæ­¥æœåŠ¡å·²åœæ­¢")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            'running': self.running,
            'targets': {
                name: {
                    'healthy': stats.is_healthy,
                    'total_synced': stats.total_synced,
                    'total_failed': stats.total_failed,
                    'last_sync_time': stats.last_sync_time,
                    'last_error': stats.last_error,
                    'consecutive_failures': stats.consecutive_failures
                }
                for name, stats in self.stats.items()
            }
        }
    
    def run(self):
        """è¿è¡ŒæœåŠ¡ï¼ˆé˜»å¡ï¼‰"""
        if not self.start():
            sys.exit(1)
        
        try:
            # ä¸»å¾ªç¯ - ç›‘æ§å’Œç»Ÿè®¡
            while self.running:
                time.sleep(60)  # æ¯åˆ†é’Ÿè¾“å‡ºä¸€æ¬¡çŠ¶æ€
                
                status = self.get_status()
                healthy_targets = sum(1 for target in status['targets'].values() if target['healthy'])
                total_targets = len(status['targets'])
                
                self.logger.info(f"æœåŠ¡çŠ¶æ€: å¥åº·ç›®æ ‡ {healthy_targets}/{total_targets}")
                
        except KeyboardInterrupt:
            self.logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·")
        finally:
            self.stop()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RedisåŒæ­¥æœåŠ¡')
    parser.add_argument('--config', '-c', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.config).exists():
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)
    
    # åˆ›å»ºå¹¶è¿è¡ŒæœåŠ¡
    service = RedisSyncService(args.config)
    service.run()


if __name__ == '__main__':
    main()
